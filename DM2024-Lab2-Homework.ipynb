{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "\n",
    "Name: Samuel Dharma\n",
    "\n",
    "Student ID: 110006223\n",
    "\n",
    "GitHub ID: 127833575\n",
    "\n",
    "Kaggle name: samueldharma\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: **This part is worth 30% of your grade.** Do the **take home exercises** in the [DM2024-Lab2-master Repo](https://github.com/didiersalazar/DM2024-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook.\n",
    "\n",
    "2. Second: **This part is worth 30% of your grade.** Participate in the in-class [Kaggle Competition](https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework) regarding Emotion Recognition on Twitter by this link: https://www.kaggle.com/competitions/dm-2024-isa-5810-lab-2-homework. The scoring will be given according to your place in the Private Leaderboard ranking:\n",
    "\n",
    "   - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "   - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) _ 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 _ 100 + 1 - 3) / (0.6 _ 100) _ 10 + 20 = 29.67% out of 30%.)  \n",
    "     Submit your last submission **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "3. Third: **This part is worth 30% of your grade.** A report of your work developing the model for the competition (You can use code and comment on it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained.\n",
    "\n",
    "4. Fourth: **This part is worth 10% of your grade.** It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook**.\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository **BEFORE the deadline (Nov. 26th, 11:59 pm, Tuesday)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports and Libraries**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found."
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _multiarray_umath: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _multiarray_umath: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Loading and Preprocessing**\n",
    "\n",
    "1. **Load Data**:\n",
    "\n",
    "   - **Tweets Dataset**: Extract `tweet_id`, `text`, and `hashtags` from `tweets_DM.json` into a DataFrame.\n",
    "   - **Emotion Labels**: Load `emotion.csv` containing emotion annotations.\n",
    "   - **Identification Labels**: Load `data_identification.csv` indicating training or test split.\n",
    "\n",
    "2. **Merge Datasets**:\n",
    "\n",
    "   - Combine tweets with identification and emotion labels using `tweet_id`.\n",
    "\n",
    "3. **Split Data**:\n",
    "   - **`train_df`**: Tweets marked for training.\n",
    "   - **`test_df`**: Tweets marked for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (1455563, 5)\n",
      "Test set: (411972, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id                                               text  \\\n",
       "0  0x376b20  People who post \"add me on #Snapchat\" must be ...   \n",
       "1  0x2d5350  @brianklaas As we see, Trump is dangerous to #...   \n",
       "3  0x1cd5b0                Now ISSA is stalking Tasha 😂😂😂 <LH>   \n",
       "5  0x1d755c  @RISKshow @TheKevinAllison Thx for the BEST TI...   \n",
       "6  0x2c91a8       Still waiting on those supplies Liscus. <LH>   \n",
       "\n",
       "                        hashtags identification       emotion  \n",
       "0                     [Snapchat]          train  anticipation  \n",
       "1  [freepress, TrumpLegacy, CNN]          train       sadness  \n",
       "3                             []          train          fear  \n",
       "5      [authentic, LaughOutLoud]          train           joy  \n",
       "6                             []          train  anticipation  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Begin Assignment Here\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the JSON file containing tweets\n",
    "tweets = []\n",
    "with open('./data/tweets_DM.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tweets.append(json.loads(line))\n",
    "\n",
    "tweets_df = pd.DataFrame([{\n",
    "    'tweet_id': item['_source']['tweet']['tweet_id'],\n",
    "    'text': item['_source']['tweet']['text'],\n",
    "    'hashtags': item['_source']['tweet']['hashtags']\n",
    "} for item in tweets])\n",
    "\n",
    "# Load the emotion and identification datasets\n",
    "emotion_df = pd.read_csv('./data/emotion.csv')\n",
    "identification_df = pd.read_csv('./data/data_identification.csv')\n",
    "\n",
    "# Merge datasets\n",
    "merged_df = pd.merge(tweets_df, identification_df, on='tweet_id')\n",
    "merged_df = pd.merge(merged_df, emotion_df, on='tweet_id', how='left')\n",
    "\n",
    "# Separate train and test data\n",
    "train_df = merged_df[merged_df['identification'] == 'train']\n",
    "test_df = merged_df[merged_df['identification'] == 'test']\n",
    "\n",
    "print(f\"Train set: {train_df.shape}\")\n",
    "print(f\"Test set: {test_df.shape}\")\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Text Preprocessing**\n",
    "\n",
    "1. **Import Libraries**:\n",
    "\n",
    "   - **`re`**: For text cleaning using regular expressions.\n",
    "   - **NLTK Tools**:\n",
    "     - `word_tokenize`: Tokenizes text into words.\n",
    "     - `stopwords`: Provides a list of common English stopwords.\n",
    "     - `WordNetLemmatizer`: Lemmatizes words to their base forms.\n",
    "\n",
    "2. **Download NLTK Data**:\n",
    "\n",
    "   - Ensure the required NLTK datasets (`punkt`, `stopwords`, `wordnet`) are downloaded.\n",
    "\n",
    "3. **Text Cleaning Function**:\n",
    "\n",
    "   - **Purpose**: Prepares text for analysis by removing noise and standardizing content.\n",
    "   - **Steps**:\n",
    "     - Remove specific tokens (`<LH>`, hashtags, mentions, and special characters).\n",
    "     - Convert text to lowercase.\n",
    "     - Tokenize into words.\n",
    "     - Remove stopwords and lemmatize tokens.\n",
    "     - Return cleaned text as a single string.\n",
    "\n",
    "4. **Apply Cleaning**:\n",
    "   - **Train Data**: Clean the `text` column in `train_df` and save it as a new column `clean_text`.\n",
    "   - **Test Data**: Perform the same cleaning process for the `text` column in `test_df`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Samuel\n",
      "[nltk_data]     Sukatja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Samuel\n",
      "[nltk_data]     Sukatja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Samuel\n",
      "[nltk_data]     Sukatja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\Users\\Samuel Sukatja\\AppData\\Local\\Temp\\ipykernel_17904\\2435772633.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
      "C:\\Users\\Samuel Sukatja\\AppData\\Local\\Temp\\ipykernel_17904\\2435772633.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['clean_text'] = test_df['text'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Text cleaning function\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'<LH>', '', text)  # Remove <LH>\n",
    "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Lemmatize and remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply cleaning to train and test sets\n",
    "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
    "test_df['clean_text'] = test_df['text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I count the total vocabulary size of the 'clean_text'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 246719\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(train_df['clean_text'])\n",
    "vocab_size = len(vectorizer.get_feature_names_out())\n",
    "print(f\"Total Vocabulary Size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model + Feature Engineering**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bag-of-Words and Hyperparameter Tuning + MNB**\n",
    "\n",
    "1. **Data Split**:\n",
    "\n",
    "   - Split `train_df` into training and validation sets using `train_test_split` (80% train, 20% validation).\n",
    "\n",
    "2. **Pipeline**:\n",
    "\n",
    "   - **`CountVectorizer`**: Converts text to Bag-of-Words features.\n",
    "   - **`MultinomialNB`**: Naive Bayes classifier.\n",
    "\n",
    "3. **Parameter Grid**:\n",
    "\n",
    "   - Tune `max_features` in `CountVectorizer` with values: `[100000, 150000, 200000, 250000, 500000, 1000000]`.\n",
    "\n",
    "4. **Grid Search**:\n",
    "\n",
    "   - Perform 5-fold cross-validation to find the best `max_features` using `GridSearchCV`.\n",
    "   - Optimize for `accuracy` and log progress with `verbose=1`.\n",
    "\n",
    "5. **Results**:\n",
    "   - Display the best parameter (`max_features`) and the highest cross-validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best Parameters: {'vectorizer__max_features': 200000}\n",
      "Best Cross-Validation Accuracy: 0.4700\n"
     ]
    }
   ],
   "source": [
    "# Create validation set from train_df\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df['clean_text'], train_df['emotion'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for max_features\n",
    "param_grid = {\n",
    "    'vectorizer__max_features': [100000, 150000, 200000, 250000, 500000, 1000000]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    verbose=1, \n",
    "    n_jobs=-1, \n",
    "    return_train_score=True  # Ensure this is set to include training scores\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features | Training Accuracy | Validation Accuracy\n",
      "100000       | 0.5089           | 0.4685\n",
      "150000       | 0.5166           | 0.4697\n",
      "200000       | 0.5198           | 0.4700\n",
      "250000       | 0.5198           | 0.4700\n",
      "500000       | 0.5198           | 0.4700\n",
      "1000000      | 0.5198           | 0.4700\n"
     ]
    }
   ],
   "source": [
    "# Retrieve results\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Extract training and validation accuracies for each max_features value\n",
    "train_scores = results['mean_train_score']\n",
    "val_scores = results['mean_test_score']\n",
    "max_features = param_grid['vectorizer__max_features']\n",
    "\n",
    "# Output accuracies\n",
    "print(\"max_features | Training Accuracy | Validation Accuracy\")\n",
    "for mf, train_acc, val_acc in zip(max_features, train_scores, val_scores):\n",
    "    print(f\"{mf:<12} | {train_acc:.4f}           | {val_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    "- Training accuracy stabilizes at higher `max_features` values.\n",
    "- Validation accuracy peaks at 200,000 and does not improve beyond this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Evaluation and Results**\n",
    "\n",
    "1. **Best Parameters**:\n",
    "\n",
    "   - Optimal `max_features`: **200,000**\n",
    "   - Best Cross-Validation Accuracy: **0.4700**\n",
    "\n",
    "2. **Validation Set Performance**:\n",
    "\n",
    "   - **Model**: Evaluate the best pipeline on the validation set using `accuracy_score` and `classification_report`.\n",
    "\n",
    "3. **Performance Metrics**:\n",
    "   - **Accuracy**: **0.4709**\n",
    "   - **Class-wise Metrics**:\n",
    "     - Precision, recall, and F1-score for each emotion (high recall for `joy` but lower performance for `anger` and `surprise`).\n",
    "   - **Macro Avg**: **F1-score: 0.34**, highlighting class imbalance challenges.\n",
    "   - **Weighted Avg**: **F1-score: 0.44**, reflecting overall model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'vectorizer__max_features': 200000}\n",
      "Best Cross-Validation Accuracy: 0.4700\n",
      "\n",
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.77      0.08      0.15      7946\n",
      "anticipation       0.52      0.47      0.49     49984\n",
      "     disgust       0.37      0.28      0.32     27669\n",
      "        fear       0.72      0.21      0.33     12846\n",
      "         joy       0.48      0.78      0.59    102943\n",
      "     sadness       0.38      0.37      0.38     38745\n",
      "    surprise       0.79      0.11      0.19      9816\n",
      "       trust       0.46      0.17      0.25     41164\n",
      "\n",
      "    accuracy                           0.47    291113\n",
      "   macro avg       0.56      0.31      0.34    291113\n",
      "weighted avg       0.49      0.47      0.44    291113\n",
      "\n",
      "Validation Accuracy: 0.4709\n"
     ]
    }
   ],
   "source": [
    "# Best results\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate the best model on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "validation_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TF-IDF Vectorizer with Hyperparameter Tuning + MNB**\n",
    "\n",
    "1. **Pipeline**:\n",
    "\n",
    "   - Uses `TfidfVectorizer` for text representation and `MultinomialNB` for classification.\n",
    "\n",
    "2. **Parameter Grid**:\n",
    "\n",
    "   - Tests combinations of `ngram_range`, `min_df`, `max_df`, `max_features`, and `alpha`.\n",
    "\n",
    "3. **Results**:\n",
    "\n",
    "   - **Best Parameters**: Optimal configuration for TF-IDF and Naive Bayes.\n",
    "   - **Best Cross-Validation Accuracy**: Reported during grid search.\n",
    "\n",
    "4. **Validation Performance**:\n",
    "   - **Accuracy**: Displays the validation accuracy.\n",
    "   - **Classification Report**: Shows precision, recall, and F1-score for each emotion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Samuel Sukatja\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'nb__alpha': 0.5, 'tfidf__max_df': 0.5, 'tfidf__max_features': 200000, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 3)}\n",
      "Best Cross-Validation Accuracy: 0.4869\n",
      "\n",
      "Validation Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.92      0.09      0.16      7946\n",
      "anticipation       0.60      0.46      0.52     49984\n",
      "     disgust       0.40      0.26      0.31     27669\n",
      "        fear       0.78      0.22      0.35     12846\n",
      "         joy       0.48      0.83      0.61    102943\n",
      "     sadness       0.40      0.38      0.39     38745\n",
      "    surprise       0.93      0.11      0.20      9816\n",
      "       trust       0.53      0.19      0.28     41164\n",
      "\n",
      "    accuracy                           0.49    291113\n",
      "   macro avg       0.63      0.32      0.35    291113\n",
      "weighted avg       0.53      0.49      0.45    291113\n",
      "\n",
      "Validation Accuracy: 0.4897\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2), (1,3)],  # Unigrams or unigrams + bigrams\n",
    "    'tfidf__max_features': [200000],  # Vocabulary size\n",
    "    'tfidf__min_df': [1, 5, 10],  # Minimum document frequency for words\n",
    "    'tfidf__max_df': [0.5, 0.75, 1.0],  # Maximum document frequency\n",
    "    'nb__alpha': [0.1, 0.5, 1.0]  # Laplace smoothing parameter\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and scores\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on the validation set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "validation_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TF-IDF Model Results**\n",
    "\n",
    "1. **Grid Search Process**:\n",
    "\n",
    "   - Evaluated 162 parameter combinations across 5 folds, resulting in 810 fits.\n",
    "   - Encountered a runtime warning but completed the search.\n",
    "\n",
    "2. **Best Parameters**:\n",
    "\n",
    "   ```\n",
    "   {'nb__alpha': 0.5, 'tfidf__max_df': 0.5, 'tfidf__max_features': 200000, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 3)}\n",
    "   ```\n",
    "\n",
    "3. **Performance**:\n",
    "\n",
    "   - **Cross-Validation Accuracy**: **0.4869**\n",
    "   - **Validation Accuracy**: **0.4897**\n",
    "\n",
    "4. **Validation Classification Report**:\n",
    "   - **Strengths**: High precision for `anger` and `surprise`.\n",
    "   - **Weaknesses**: Low recall for most emotions except `joy`.\n",
    "   - **Macro Avg F1-Score**: **0.35**\n",
    "   - **Weighted Avg F1-Score**: **0.45**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TF-IDF + MNB with Unprocessed Text**\n",
    "\n",
    "1. Now we want to test model performance using raw text without preprocessing.\n",
    "\n",
    "2. **Process**:\n",
    "   - Split raw text into training and validation sets.\n",
    "   - Use `TfidfVectorizer` with optimal parameters from grid search.\n",
    "   - Train `MultinomialNB` with `alpha=0.5`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.5952\n",
      "Validation Accuracy: 0.5455\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Create validation set from train_df\n",
    "X_train_raw, X_val_raw, y_train, y_val = train_test_split(\n",
    "    train_df['text'], train_df['emotion'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Extract features using TfidfVectorizer with best parameters\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, \n",
    "    max_features=235020, \n",
    "    min_df= 1, \n",
    "    ngram_range=(1, 3)\n",
    ")\n",
    "\n",
    "# Transform the training and validation data\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_val = vectorizer.transform(X_val_raw)\n",
    "\n",
    "# Train MultinomialNB with the best alpha\n",
    "model = MultinomialNB(alpha=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate and print training accuracy\n",
    "y_train_pred = model.predict(X_train)\n",
    "training_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy: {training_accuracy:.4f}\")\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "y_val_pred = model.predict(X_val)\n",
    "validation_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, the model performs better with unprocessed text, meaning that raw text may retain valuable context for this dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Final Model Training and Test Prediction**\n",
    "\n",
    "1. Now we train the model on the entire training dataset and make predictions on the test dataset.\n",
    "\n",
    "2. **Process**:\n",
    "\n",
    "   - **Training Data**:\n",
    "     - Text: `train_df['text']`\n",
    "     - Labels: `train_df['emotion']`\n",
    "   - **Test Data**:\n",
    "     - Text: `test_df['text']`\n",
    "     - IDs: `test_df['tweet_id']`\n",
    "   - **Vectorization**: Use `TfidfVectorizer` with:\n",
    "     - Parameters: `max_df=0.5`, `max_features=235020`, `min_df=1`, `ngram_range=(1, 3)`\n",
    "     - Additional: Stop word removal and sublinear term frequency scaling.\n",
    "   - **Model**: Train `MultinomialNB` with `alpha=0.5`.\n",
    "\n",
    "3. **Results**:\n",
    "\n",
    "   - **Training Accuracy**: Evaluate the model on the training set.\n",
    "   - **Test Predictions**: Generate emotion predictions for the test set.\n",
    "\n",
    "4. **Submission**:\n",
    "   Create a submission file (`submission.csv`) with columns:\n",
    "   - `id`: Tweet IDs from the test dataset.\n",
    "   - `emotion`: Predicted emotion labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.5938\n",
      "Submission file 'submission.csv' created!\n"
     ]
    }
   ],
   "source": [
    "# Extract training data\n",
    "X_train_raw = train_df['text']\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "# Extract test data\n",
    "X_test_raw = test_df['text']\n",
    "test_ids = test_df['tweet_id']\n",
    "\n",
    "# Vectorize the text data using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    max_features=235020,\n",
    "    min_df=1,\n",
    "    ngram_range=(1, 3),\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "# Transform the training and test data\n",
    "X_train = vectorizer.fit_transform(X_train_raw)\n",
    "X_test = vectorizer.transform(X_test_raw)\n",
    "\n",
    "# Train the MultinomialNB model\n",
    "model = MultinomialNB(alpha=0.5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "y_train_pred = model.predict(X_train)\n",
    "training_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(f\"Training Accuracy: {training_accuracy:.4f}\")\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Prepare the submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'emotion': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file 'submission.csv' created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the Results of Training Accuracy is 0.5938 and Submission file (submission.csv) generated with predictions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
